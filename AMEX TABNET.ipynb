{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4d5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import random\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abba16c5-1e6c-4884-818a-5d2a1404971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458913, 2033)\n",
      "(458913, 2)\n"
     ]
    }
   ],
   "source": [
    "#skip make_data\n",
    "data_use=pd.read_feather(\"../Kaggle/AMEX/train_use.ftr\")\n",
    "train_labels=pd.read_csv(\"../Kaggle/AMEX/train_labels.csv\")\n",
    "print(data_use.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "def categorization(data):\n",
    "\n",
    "    num_cols=data._get_numeric_data().columns\n",
    "    cat_cols=list(set(data.columns) - set(num_cols))\n",
    "\n",
    "    for column in cat_cols:\n",
    "        target_column = data[column]\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(target_column)\n",
    "        label_encoded_column = le.transform(target_column)\n",
    "        data[column] = pd.Series(label_encoded_column).astype('category')\n",
    "    \n",
    "    return data\n",
    "\n",
    "data_use=categorization(data_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb151e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_use=data_use.iloc[0:1000,0:40]\n",
    "#train_labels=train_labels.iloc[0:1000,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2f0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEBUG = False\n",
    "    model = 'tabnet'\n",
    "    N_folds = 5\n",
    "    seed = 42\n",
    "    batch_size = 512\n",
    "    max_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36934005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(seed = CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b86ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric_numpy(y_true: np.array, y_pred: np.array) -> float:\n",
    "\n",
    "    # count of positives and negatives\n",
    "    n_pos = y_true.sum()\n",
    "    n_neg = y_true.shape[0] - n_pos\n",
    "\n",
    "    # sorting by descring prediction values\n",
    "    indices = np.argsort(y_pred)[::-1]\n",
    "    preds, target = y_pred[indices], y_true[indices]\n",
    "\n",
    "    # filter the top 4% by cumulative row weights\n",
    "    weight = 20.0 - target * 19.0\n",
    "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
    "    four_pct_filter = cum_norm_weight <= 0.04\n",
    "\n",
    "    # default rate captured at 4%\n",
    "    d = target[four_pct_filter].sum() / n_pos\n",
    "\n",
    "    # weighted gini coefficient\n",
    "    lorentz = (target / n_pos).cumsum()\n",
    "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
    "\n",
    "    # max weighted gini coefficient\n",
    "    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n",
    "\n",
    "    # normalized weighted gini coefficient\n",
    "    g = gini / gini_max\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c013c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amex_tabnet(Metric):\n",
    "    \n",
    "  def __init__(self):\n",
    "    self._name = 'amex_tabnet'\n",
    "    self._maximize = True\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    amex = amex_metric_numpy(y_true, y_pred[:, 1])\n",
    "    return max(amex, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f16742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 0.55506 | val_0_auc: 0.91432 | val_0_accuracy: 0.84036 | val_0_amex_tabnet: 0.62302 |  0:13:06s\n",
      "epoch 1  | loss: 0.37439 | val_0_auc: 0.928   | val_0_accuracy: 0.86273 | val_0_amex_tabnet: 0.66821 |  0:25:57s\n",
      "epoch 2  | loss: 0.33387 | val_0_auc: 0.93325 | val_0_accuracy: 0.86728 | val_0_amex_tabnet: 0.69043 |  0:38:48s\n",
      "epoch 3  | loss: 0.31245 | val_0_auc: 0.93784 | val_0_accuracy: 0.87437 | val_0_amex_tabnet: 0.70421 |  0:51:44s\n",
      "epoch 4  | loss: 0.30035 | val_0_auc: 0.94011 | val_0_accuracy: 0.87715 | val_0_amex_tabnet: 0.71328 |  1:04:49s\n",
      "epoch 5  | loss: 0.28304 | val_0_auc: 0.94783 | val_0_accuracy: 0.88647 | val_0_amex_tabnet: 0.74411 |  1:17:39s\n",
      "epoch 6  | loss: 0.26351 | val_0_auc: 0.95032 | val_0_accuracy: 0.88977 | val_0_amex_tabnet: 0.75083 |  1:30:18s\n",
      "epoch 7  | loss: 0.25625 | val_0_auc: 0.95238 | val_0_accuracy: 0.89125 | val_0_amex_tabnet: 0.75887 |  1:43:25s\n",
      "epoch 8  | loss: 0.25137 | val_0_auc: 0.95313 | val_0_accuracy: 0.89336 | val_0_amex_tabnet: 0.76132 |  1:56:32s\n",
      "epoch 9  | loss: 0.24875 | val_0_auc: 0.95345 | val_0_accuracy: 0.89381 | val_0_amex_tabnet: 0.76198 |  2:09:59s\n",
      "epoch 10 | loss: 0.24989 | val_0_auc: 0.95404 | val_0_accuracy: 0.89466 | val_0_amex_tabnet: 0.76621 |  2:23:22s\n",
      "epoch 11 | loss: 0.24388 | val_0_auc: 0.95732 | val_0_accuracy: 0.89874 | val_0_amex_tabnet: 0.77805 |  2:36:55s\n",
      "epoch 12 | loss: 0.23667 | val_0_auc: 0.95859 | val_0_accuracy: 0.89941 | val_0_amex_tabnet: 0.78153 |  2:50:40s\n",
      "epoch 13 | loss: 0.23264 | val_0_auc: 0.95891 | val_0_accuracy: 0.90046 | val_0_amex_tabnet: 0.78422 |  3:04:15s\n",
      "epoch 14 | loss: 0.22852 | val_0_auc: 0.96049 | val_0_accuracy: 0.90199 | val_0_amex_tabnet: 0.79045 |  3:18:11s\n",
      "epoch 15 | loss: 0.23507 | val_0_auc: 0.9598  | val_0_accuracy: 0.90184 | val_0_amex_tabnet: 0.78712 |  6:09:19s\n",
      "epoch 16 | loss: 0.23211 | val_0_auc: 0.96005 | val_0_accuracy: 0.89896 | val_0_amex_tabnet: 0.78833 |  6:31:03s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rikuta/Kaggle/AMEX TABNET.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=25'>26</a>\u001b[0m model \u001b[39m=\u001b[39m TabNetClassifier(n_d \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=26'>27</a>\u001b[0m                          n_a \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=27'>28</a>\u001b[0m                          n_steps \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=41'>42</a>\u001b[0m                          mask_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mentmax\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=42'>43</a>\u001b[0m                          seed \u001b[39m=\u001b[39m CFG\u001b[39m.\u001b[39mseed)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=46'>47</a>\u001b[0m \u001b[39m## train\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=47'>48</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(X_train),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=48'>49</a>\u001b[0m           np\u001b[39m.\u001b[39;49marray(y_train\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mravel()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=49'>50</a>\u001b[0m           eval_set \u001b[39m=\u001b[39;49m [(np\u001b[39m.\u001b[39;49marray(X_valid), np\u001b[39m.\u001b[39;49marray(y_valid\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mravel()))],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=50'>51</a>\u001b[0m           max_epochs \u001b[39m=\u001b[39;49m CFG\u001b[39m.\u001b[39;49mmax_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=51'>52</a>\u001b[0m           patience \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=52'>53</a>\u001b[0m           batch_size \u001b[39m=\u001b[39;49m CFG\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=53'>54</a>\u001b[0m           eval_metric \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mauc\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, Amex_tabnet]) \u001b[39m# Last metric is used for early stopping\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=55'>56</a>\u001b[0m \u001b[39m# Saving best model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rikuta/Kaggle/AMEX%20TABNET.ipynb#ch0000023?line=56'>57</a>\u001b[0m saving_path_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./fold\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:223\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 223\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:434\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 434\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    438\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:469\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m    467\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[1;32m    471\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, y)\n\u001b[1;32m    472\u001b[0m \u001b[39m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:532\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    534\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:583\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    582\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[0;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:532\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    534\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:468\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    467\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 468\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    469\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    471\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[1;32m    472\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:532\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    534\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:160\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    158\u001b[0m steps_output \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_steps):\n\u001b[0;32m--> 160\u001b[0m     M \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_transformers[step](prior, att)\n\u001b[1;32m    161\u001b[0m     M_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\n\u001b[1;32m    162\u001b[0m         torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mmul(M, torch\u001b[39m.\u001b[39mlog(M \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m     \u001b[39m# update prior\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:532\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    534\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:637\u001b[0m, in \u001b[0;36mAttentiveTransformer.forward\u001b[0;34m(self, priors, processed_feat)\u001b[0m\n\u001b[1;32m    635\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(x)\n\u001b[1;32m    636\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmul(x, priors)\n\u001b[0;32m--> 637\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselector(x)\n\u001b[1;32m    638\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/torch/nn/modules/module.py:532\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 532\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m    534\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/sparsemax.py:204\u001b[0m, in \u001b[0;36mEntmax15.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m entmax15(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/sparsemax.py:127\u001b[0m, in \u001b[0;36mEntmax15Function.forward\u001b[0;34m(ctx, input, dim)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m-\u001b[39m max_val  \u001b[39m# same numerical stability trick as for softmax\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# divide by 2 to solve actual Entmax\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m tau_star, _ \u001b[39m=\u001b[39m Entmax15Function\u001b[39m.\u001b[39;49m_threshold_and_support(\u001b[39minput\u001b[39;49m, dim)\n\u001b[1;32m    128\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(\u001b[39minput\u001b[39m \u001b[39m-\u001b[39m tau_star, \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    129\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(output)\n",
      "File \u001b[0;32m~/miniforge3/envs/env/lib/python3.8/site-packages/pytorch_tabnet/sparsemax.py:147\u001b[0m, in \u001b[0;36mEntmax15Function._threshold_and_support\u001b[0;34m(input, dim)\u001b[0m\n\u001b[1;32m    144\u001b[0m Xsrt, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(\u001b[39minput\u001b[39m, descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dim\u001b[39m=\u001b[39mdim)\n\u001b[1;32m    146\u001b[0m rho \u001b[39m=\u001b[39m _make_ix_like(\u001b[39minput\u001b[39m, dim)\n\u001b[0;32m--> 147\u001b[0m mean \u001b[39m=\u001b[39m Xsrt\u001b[39m.\u001b[39;49mcumsum(dim) \u001b[39m/\u001b[39m rho\n\u001b[1;32m    148\u001b[0m mean_sq \u001b[39m=\u001b[39m (Xsrt \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcumsum(dim) \u001b[39m/\u001b[39m rho\n\u001b[1;32m    149\u001b[0m ss \u001b[39m=\u001b[39m rho \u001b[39m*\u001b[39m (mean_sq \u001b[39m-\u001b[39m mean \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TabNet\n",
    "# Create out of folds array\n",
    "oof_predictions = np.zeros((data_use.shape[0]))\n",
    "#test_predictions = np.zeros(test.shape[0])\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances[\"feature\"] = data_use.columns.tolist()\n",
    "stats = pd.DataFrame()\n",
    "explain_matrices = []\n",
    "masks_ =[]\n",
    "\n",
    "\n",
    "    \n",
    "kfold = StratifiedKFold(n_splits = CFG.N_folds, shuffle=True, random_state = CFG.seed)\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kfold.split(data_use, train_labels[\"target\"])):\n",
    "\n",
    "    ## DEBUG MODE\n",
    "    if CFG.DEBUG == True:\n",
    "        if fold > 0:\n",
    "            print('\\nDEBUG mode activated: Will train only one fold...\\n')\n",
    "            break      \n",
    "\n",
    "    X_train, y_train = data_use.iloc[train_idx,:], train_labels[\"target\"].iloc[train_idx]\n",
    "    X_valid, y_valid = data_use.iloc[valid_idx,:], train_labels[\"target\"].iloc[valid_idx]     \n",
    "        \n",
    "    model = TabNetClassifier(n_d = 32,\n",
    "                             n_a = 32,\n",
    "                             n_steps = 3,\n",
    "                             gamma = 1.3,\n",
    "                             n_independent = 2,\n",
    "                             n_shared = 2,\n",
    "                             momentum = 0.02,\n",
    "                             clip_value = None,\n",
    "                             lambda_sparse = 1e-3,\n",
    "                             optimizer_fn = torch.optim.Adam,\n",
    "                             optimizer_params = dict(lr = 1e-3, weight_decay=1e-3),\n",
    "                             scheduler_fn = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "                             scheduler_params = {'T_0':5,\n",
    "                                                 'eta_min':1e-4,\n",
    "                                                 'T_mult':1,\n",
    "                                                 'last_epoch':-1},\n",
    "                             mask_type = 'entmax',\n",
    "                             seed = CFG.seed)\n",
    "    \n",
    "    \n",
    "\n",
    "    ## train\n",
    "    model.fit(np.array(X_train),\n",
    "              np.array(y_train.values.ravel()),\n",
    "              eval_set = [(np.array(X_valid), np.array(y_valid.values.ravel()))],\n",
    "              max_epochs = CFG.max_epochs,\n",
    "              patience = 50,\n",
    "              batch_size = CFG.batch_size,\n",
    "              eval_metric = ['auc', 'accuracy', Amex_tabnet]) # Last metric is used for early stopping\n",
    "    \n",
    "    # Saving best model\n",
    "    saving_path_name = f\"./fold{fold}\"\n",
    "    saved_filepath = model.save_model(saving_path_name)\n",
    "    \n",
    "    # model explanability\n",
    "    explain_matrix, masks = model.explain(X_valid.values)\n",
    "    explain_matrices.append(explain_matrix)\n",
    "    masks_.append(masks[0])\n",
    "    masks_.append(masks[1])\n",
    "    \n",
    "    # Inference\n",
    "    oof_predictions[valid_idx] = model.predict_proba(X_valid.values)[:, 1]\n",
    "    \n",
    "    #if CFG\n",
    "    # logodds function\n",
    "    \n",
    "    #test_predictions += model.predict_proba(test.values)[:, 1]/5\n",
    "    feature_importances[f\"importance_fold{fold}+1\"] = model.feature_importances_\n",
    "    \n",
    "    # Loss , metric tracking\n",
    "    stats[f'fold{fold+1}_train_loss'] = model.history['loss']\n",
    "    stats[f'fold{fold+1}_val_metric'] = model.history['val_0_amex_tabnet']\n",
    "\n",
    "\n",
    "     \n",
    "    print(f'\\nFold {fold+1}/{CFG.N_folds}' )\n",
    "\n",
    "    ### free memory\n",
    "    del X_train, y_train\n",
    "    del X_valid, y_valid\n",
    "    gc.collect()\n",
    "\n",
    "print(f'OOF score across folds: {amex_metric_numpy(train_labels[\"target\"], oof_predictions.flatten())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fb54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4da8eb4402250ad25264d8ad4ce1c325063698da828d20e26522c6fdb53f3b79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
